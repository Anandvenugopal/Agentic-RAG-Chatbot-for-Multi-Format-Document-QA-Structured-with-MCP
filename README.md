
# Agentic RAG Chatbot for Multi-Format Document QA Structured with MCP


This project implements a sophisticated, agent-based Retrieval-Augmented Generation (RAG) chatbot. It allows users to upload documents in various formats, ask questions about their content, and receive accurate answers generated by Llama3 running on the high-speed Groq API. The system is built with a modular, agentic architecture where each component communicates via a structured Model Context Protocol (MCP).

## ✨ Features

- ✅ **Multi-Format Document Upload**: Ingest and process `.pdf`, `.docx`, `.pptx`, `.csv`, `.txt`, and `.md` files.
- ✅ **Multi-Document Knowledge Base**: Upload multiple files to create a combined knowledge base for the chat session.
- 🧠 **Agentic Architecture**: The system is composed of three distinct agents, each with a specific role:
  - `IngestionAgent`: Parses and chunks documents.
  - `RetrievalAgent`: Creates embeddings and retrieves context from the vector DB.
  - `LLMResponseAgent`: Generates final answers based on context.
- ⚡ **High-Speed Inference**: Utilizes Llama3 via the **Groq API** for extremely fast response generation.
- 🗂️ **Vector Storage**: Leverages **Pinecone** as the vector database for efficient semantic search.
- 📜 **Source-Cited Answers**: Responses include expandable source context, showing the exact text chunks from the original documents used to formulate the answer.
- 🎨 **Customizable UI**: A clean and responsive user interface built with **Streamlit**, with a centralized styling system for easy customization.
- 🔄 **Model Context Protocol (MCP)**: All communication between agents follows a structured JSON-based protocol for clarity and traceability.

## 🛠️ Tech Stack

- **LLM**: Llama3 via Groq
- **Embeddings**: `all-MiniLM-L6-v2` (from Sentence-Transformers)
- **Vector Database**: Pinecone
- **Application Framework**: Streamlit
- **Core Libraries**: LangChain, Unstructured.io

---

## 🚀 Setup and Installation

### 1. Prerequisites

- Python 3.9 or higher
- Git

### 2. Clone the Repository

```bash
git clone <your-repository-url>
cd agentic-rag-chatbot
```

### 3. Set Up a Virtual Environment

**On Windows:**
```bash
python -m venv venv
.venv\Scripts\activate
```

**On macOS/Linux:**
```bash
python -m venv venv
source venv/bin/activate
```

### 4. Create and Populate `requirements.txt`

Create a file named `requirements.txt` in the project root and add the following content:
```text
# requirements.txt
langchain
langchain-core
langchain-community
groq
pinecone-client
python-dotenv
streamlit
langchain-groq
sentence-transformers
unstructured[all-docs]
```

### 5. Install Dependencies

```bash
pip install -r requirements.txt
```

> Note: `unstructured[all-docs]` might take a few minutes to install as it includes many parsing libraries.

### 6. Configure API Keys

Create a file named `.env` in the root of your project directory and add:
```bash
GROQ_API_KEY="gsk_YourGroqApiKey"
PINECONE_API_KEY="YourPineconeApiKey"
```

### 7. Set Up Your Pinecone Index

Create a Pinecone index with these specifications:
- Name: `agentic-rag-index`
- Dimensions: `384`
- Metric: `cosine`

### ▶️ Run the Application

```bash
streamlit run app.py
```

### 📖 How to Use

- Upload documents using the file uploader.
- Click "Process and Add to Knowledge Base".
- Start chatting with context-aware questions.
- Use "View Source Context" to see source text.
- Click "Clear Knowledge Base" to reset.

## 📁 Project Structure

```text
agentic-rag-chatbot/
├── .env                                           # API keys (not committed)
├── app.py                                         # Streamlit app
├── mcp.py                                         # MCP message structure
├── ingestion_agent.py                             # Parsing & chunking
├── retrieval_agent.py                             # Embedding & retrieval
├── llm_response_agent.py                          # Answer generation
├── orchestrator.py                                # Workflow management
├──Agent-Based-Architecture-with-MCP-Integration   # presntation
└── requirements.txt                               # Python dependencies
```
